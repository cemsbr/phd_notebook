{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initial setup\n",
    "\n",
    "try:\n",
    "    plt_inline\n",
    "except NameError:\n",
    "    # Avoid small fonts when inlining again\n",
    "    %matplotlib inline\n",
    "    plt_inline = True\n",
    "    \n",
    "from inc.notebook001 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740 logs/executions read.\n",
      "CPU times: user 22.2 s, sys: 1.41 s, total: 23.6 s\n",
      "Wall time: 23.6 s\n"
     ]
    }
   ],
   "source": [
    "## Load K-means experiments\n",
    "parser = SparkParser()\n",
    "apps = tuple(parser.parse_folder('data/hibench/kmeans'))\n",
    "print('{} logs/executions read.'.format(len(apps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## When data is read from HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among all the methods of reading data below, *Memory* is much faster then *Hadoop* or *Network*. At first, the duration prediction will deal with memory-only access. In order to do that, we will predict when data is read from *Hadoop* or *Network*. The prediction is based on the input size per worker of the profiling executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Network', 'Hadoop', None, 'Memory'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Different methods of reading data\n",
    "set(t.metrics.data_read_method\n",
    "    for a in apps\n",
    "        for s in a.stages\n",
    "            for t in s.successful_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input size is 784.45 MB/worker, data may not fit in memory. Based on this threshold, the table below shows the maximum number of workers that causes access to the disk or network.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max workers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>samples</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4096000</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16384000</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65536000</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          max workers\n",
       "samples              \n",
       "4096000             1\n",
       "16384000            4\n",
       "65536000           16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def not_from_memory(stage):\n",
    "    \"\"\"Return True if data was read from Hadoop or Network.\"\"\"\n",
    "    for task in stage.successful_tasks:\n",
    "        if task.metrics.data_read_method in ['Hadoop', 'Network']:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_profiling(app):\n",
    "    \"\"\"Check if app is part of the profiling phase.\"\"\"\n",
    "    return app.stages[0].records_read < 16384000\n",
    "\n",
    "def get_min_size(apps):\n",
    "    \"\"\"Minimum input size per worker when memory is not enough.\"\"\"\n",
    "    min_size = None\n",
    "    for app in apps:\n",
    "        size = app.stages[0].bytes_read / app.slaves\n",
    "        for stage in app.stages[1:]:\n",
    "            if not_from_memory(stage) and (min_size is None or min_size > size):\n",
    "                min_size = size\n",
    "    return min_size\n",
    "\n",
    "def predict_non_memory(apps, min_size):\n",
    "    \"\"\"Maximum number of workers when memory is not enough.\n",
    "    \n",
    "    Args:\n",
    "        min_size (int): Minimum input size per worker that exceeds memory capacity.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for total_input, records in set(\n",
    "            (a.stages[0].bytes_read, a.stages[0].records_read)\n",
    "             for a in apps):\n",
    "        max_slaves = int(total_input/min_size)\n",
    "        if max_slaves > 0:\n",
    "            res.append((records, max_slaves))\n",
    "    return res\n",
    "                \n",
    "# Only profiling executions for prediction\n",
    "min_size = get_min_size(app for app in apps if is_profiling(app))\n",
    "print('Based on the profiling executions, data may not fit in memory when input size is'\n",
    "      ' {:.2f} MB/worker or more.'.format(min_size / 1024**2))\n",
    "\n",
    "non_memory = predict_non_memory(apps, min_size)\n",
    "pd.DataFrame.from_records(non_memory, columns=['samples', 'max workers'], index='samples'). \\\n",
    "    sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above, we should use, for example, more than 4 workers if we want 16,384,000 samples to be processed completely from memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEPRECATED SECTIONS BELOW\n",
    "## Regression on stage read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_target(app):\n",
    "    return app.stages[0].bytes_read > 8 * 10**6\n",
    "\n",
    "def get_df_records(apps):\n",
    "    ns_stages = set(len(app.stages) for app in apps)\n",
    "    assert len(ns_stages) == 1, \"{} stages found\".format(len(ns_stages))\n",
    "    n_stages = ns_stages.pop()\n",
    "    # ['workers', 'samples', 'stage0read', 'stage0written', 'stage1read', ...]\n",
    "    cols = ['workers', 'samples'] + ['stage{:d}{}'.format(stage, typ)\n",
    "                                     for stage in range(n_stages)\n",
    "                                     for typ in ('read', 'written')]\n",
    "    rows = []\n",
    "    for app in apps:\n",
    "        # samples in millions\n",
    "        row = [app.slaves, app.stages[0].records_read / 10**6]\n",
    "        for stage in app.stages:\n",
    "            # read/written Kb\n",
    "            row.extend([stage.bytes_read / 1024, stage.bytes_written / 1024])\n",
    "        rows.append(row)\n",
    "    return rows, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "records, cols = get_df_records(apps)\n",
    "df = pd.DataFrame.from_records(records, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build graphs to understand how data grows as we increase the number of samples\n",
    "# - Select a stage\n",
    "# - Consider all number of workers\n",
    "# - x: number of samples\n",
    "# - y1: data read (scatterplot)\n",
    "# - y2: data written (scatterplot)\n",
    "def plot_stage(stage_id):\n",
    "    x = 'samples'\n",
    "    y1 = 'stage{:d}read'.format(stage_id)\n",
    "    y2 = 'stage{:d}written'.format(stage_id)\n",
    "    graph_df = pd.DataFrame()\n",
    "    _df = df#[df[x] < 16]\n",
    "    graph_df[x] = _df[x]\n",
    "    graph_df[y1] = _df[y1] / 1024  # to MB\n",
    "    graph_df[y2] = _df[y2] / 1024  # to MB\n",
    "    alpha=0.5\n",
    "    \n",
    "    ax = graph_df.plot.scatter(x, y2, s=60, c='r', alpha=alpha)\n",
    "    graph_df.plot.scatter(x, y1, s=60, alpha=alpha, ax=ax)\n",
    "    \n",
    "    prof_df = graph_df[[x, y1]][graph_df.samples < 16]\n",
    "    x_max = graph_df[x].max()\n",
    "    plot_data_size_prediction(ax, prof_df, x_max)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    #list_points(graph_df[[x, y1]])\n",
    "    #list_points(graph_df[[x, y2]])\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "def plot_data_size_prediction(ax, df, x_max):\n",
    "    x, y = df.columns\n",
    "    z = np.polyfit(df[x], df[y], 1)\n",
    "    p = np.poly1d(z)\n",
    "    xs = np.linspace(0, x_max, x_max)\n",
    "    ax.plot(xs, p(xs))\n",
    "    \n",
    "from IPython.display import display\n",
    "    \n",
    "def list_points(df):\n",
    "    col1, col2 = df.columns\n",
    "    counts = df.groupby(col1).apply(lambda x: x[col2].value_counts())\n",
    "    display(counts)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "for stage in range(len(apps[0].stages)):\n",
    "    plot_stage(stage)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum data size per worker when there's an HDFS read, and maximum data size per worker when only memory is used. Presented by stage, except for stage 0 (initial data is always read from disk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def most_common(values):\n",
    "    size, count = Counter(values).most_common(1)[0]\n",
    "    return '{}/{}: {:.2f} MB'.format(count, len(values), size / 1024**2)\n",
    "\n",
    "cols = ['stage', 'slaves', 'input per worker', 'HDFS']\n",
    "records = ((stage.id, app.slaves, app.stages[0].bytes_read / app.slaves, not_from_memory(stage))\n",
    "           for app in apps\n",
    "               for stage in app.stages[1:])\n",
    "_df = pd.DataFrame.from_records(records, columns=cols)\n",
    "min_hdfs = _df[_df['HDFS']].drop('HDFS', axis=1).groupby(cols[:2]).agg(min). \\\n",
    "    rename(columns={'input per worker': 'min HDFS (MB/worker)'}) / 1024**2\n",
    "max_mem = _df[~_df['HDFS']].drop('HDFS', axis=1).groupby(cols[:2]).agg(max). \\\n",
    "    rename(columns={'input per worker': 'max mem (MB/worker)'}) / 1024**2\n",
    "\n",
    "# TODO all sizes per stage\n",
    "def unique_str(values):\n",
    "    mbs = sorted(set(round(val/1024**2, 2) for val in values), reverse=True)\n",
    "    return ', '.join(str(mb) for mb in mbs)\n",
    "\n",
    "all_sizes = _df.drop('HDFS', axis=1).groupby(cols[:2]).agg(unique_str). \\\n",
    "    rename(columns={'input per worker': 'all sizes (MB/worker)'})\n",
    "\n",
    "# More detailed table\n",
    "_df = pd.concat([max_mem, min_hdfs, all_sizes], axis=1).dropna()\n",
    "cols = ['stage', 'slaves', 'max mem (MB/worker)', 'min HDFS (MB/worker)', 'all sizes (MB/worker)']\n",
    "\n",
    "\n",
    "# # Per-stage mininum size for HDFS\n",
    "# def unique_str_split(values):\n",
    "#     all_entries = set()\n",
    "#     for val in values:\n",
    "#         all_entries.update(val.split(', '))\n",
    "#     numbers = (float(entry) for entry in all_entries)\n",
    "#     ordered = sorted(numbers, reverse=True)\n",
    "#     return ', '.join(str(n) for n in ordered)\n",
    "\n",
    "# _df.reset_index().drop(cols[1:3], axis=1).groupby('stage').agg({\n",
    "#         cols[3]: min, cols[-1]: unique_str_split})[cols[-2:]]\n",
    "\n",
    "_df[cols[-2]].min()\n",
    "\n",
    "groups = set((app.stages[0].bytes_read, app.slaves, app.stages[0].records_read) for app in apps)\n",
    "\n",
    "disk = ((records, slaves) for size, slaves, records in groups\n",
    "        if size / slaves >= min_size)\n",
    "print('Possible disk access:')\n",
    "pd.DataFrame.from_records(disk, columns=['records', 'max slaves for HDFS']).groupby('records').max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_df.iloc[:,:2].groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for app in apps:\n",
    "    stage = app.stages[13]\n",
    "    size = stage.bytes_read / app.slaves / 1024**2\n",
    "    if abs(1104.75 - size) < 0.01 and not is_read_from_hdfs(stage):\n",
    "        print(app.slaves)\n",
    "#     elif abs(842.77 - size) < 0.01 and is_read_from_hdfs(stage):\n",
    "#         print(app.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_read_from_hdfs(stage):\n",
    "    return 'Hadoop' in (t.metrics.data_read_method for t in stage.successful_tasks)\n",
    "\n",
    "def get_records(apps):\n",
    "    for app in apps:\n",
    "        input0 = app.stages[0].bytes_read\n",
    "        samples = app.stages[0].records_read\n",
    "        for stage in app.stages:\n",
    "            yield (\n",
    "                is_read_from_hdfs(stage),\n",
    "                input0,\n",
    "                samples,\n",
    "                app.slaves,\n",
    "                stage.id,\n",
    "                input0 / app.slaves / 1024**2,\n",
    "                1)\n",
    "            \n",
    "cols = ['hadoop', 'input (stg 0)', 'samples', 'slaves', 'stage', 'input/slave', 'count (out of 10)']\n",
    "_df = pd.DataFrame.from_records(get_records(apps), columns=cols).sort_values(cols[1:4])\n",
    "_df[_df.hadoop & (_df.stage > 0)].drop('hadoop', axis=1).groupby(cols[1:5]).agg({cols[5]: min, cols[6]: sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
